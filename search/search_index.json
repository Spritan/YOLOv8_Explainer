{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to YOLOv8 Explainer","text":""},{"location":"#simplify-your-understanding-of-yolov8-results","title":"Simplify your understanding of YOLOv8 Results","text":"<p>This is a package with state of the art Class Activated Mapping(CAM) methods for Explainable AI for computer vision using YOLOv8. This can be used for diagnosing model predictions, either in production or while developing models. The aim is also to serve as a benchmark of algorithms and metrics for research of new explainability methods.</p>"},{"location":"#install-environment-dependencies","title":"Install Environment &amp; Dependencies","text":"<p><code>YOLOv8-Explainer</code> can be seamlessly integrated into your projects with a straightforward installation process:</p>"},{"location":"#installation-as-a-package","title":"Installation as a Package","text":"<p>To incorporate <code>YOLOv8-Explainer</code> into your project as a dependency, execute the following command in your terminal:</p> <pre><code>pip install YOLOv8-Explainer\n</code></pre>"},{"location":"#features-and-functionality","title":"Features and Functionality","text":"<p><code>YOLOv8-Explainer</code>  can be used to deploy various different CAM models for cutting-edge XAI methodologies in <code>YOLOv8</code> for images:</p> <ul> <li>GradCAM : Weight the 2D activations by the average gradient</li> <li>GradCAM + + : Like GradCAM but uses second order gradients</li> <li>XGradCAM : Like GradCAM but scale the gradients by the normalized activations</li> <li>EigenCAM : Takes the first principle component of the 2D Activations (no class discrimination, but seems to give great results)</li> <li>HiResCAM : Like GradCAM but element-wise multiply the activations with the gradients; provably guaranteed faithfulness for certain models</li> <li>LayerCAM : Spatially weight the activations by positive gradients. Works better especially in lower layers</li> <li>EigenGradCAM : Like EigenCAM but with class discrimination: First principle component of Activations*Grad. Looks like GradCAM, but cleaner</li> </ul>"},{"location":"#using-from-code-as-a-library","title":"Using from code as a library","text":"<p><pre><code>from YOLOv8_Explainer import yolov8_heatmap, display_images\n\nmodel = yolov8_heatmap(\n    weight=\"/location/model.pt\", \n        conf_threshold=0.4,  \n        method = \"EigenCAM\", \n        layer=[10, 12, 14, 16, 18, -3],\n        ratio=0.02,\n        show_box=True,\n        renormalize=False,\n)\n\nimages = model(\n    img_path=\"/location/image.jpg\", \n    )\n\ndisplay_images(images)\n</code></pre> - Here the <code>from YOLOv8_Explainer import yolov8_heatmap, display_images</code> allows you to import the required functionalities. </p> <ul> <li>The line <code>model = yolov8_heatmap( weight=\"/location/model.pt\", conf_threshold=0.4, method = \"EigenCAM\", layer=[12, 17, 21], ratio=0.02, show_box=True, renormalize=False)</code> allows the user to pass a pertrained YOLO weight which the CAM is expected to evaluate, along with additional parameters like the desired CAM method target layers, and the confidence threshold.</li> </ul> <p>You can choose between the following CAM Models for version 0.0.5:</p> <p><code>GradCAM</code> , <code>HiResCAM</code>, <code>GradCAMPlusPlus</code>, <code>XGradCAM</code> , <code>LayerCAM</code>, <code>EigenGradCAM</code> and <code>EigenCAM</code>.</p> <ul> <li> <p>The line <code>images = model( img_path=\"/location/image.jpg\" )</code> passes the images the model will process</p> </li> <li> <p>The line <code>display_images(images)</code> displays the output of the model (bounding box), along with the CAM model's output (saliency map). </p> </li> </ul> <p>You can add a single image or a directory images to be used by the <code>Module</code>. The output will be a corresponding list of images (list containing one PIL Image for a single image input and list containing as many PIL images as Images in the input directory).</p>"},{"location":"#citing-in-works","title":"Citing in Works","text":"<p>If you use this for research, please cite. Here is an example BibTeX entry:</p> <pre><code>@ARTICLE{Sarma_Borah2024-un,\n  title     = \"A comprehensive study on Explainable {AI} using {YOLO} and post\n               hoc method on medical diagnosis\",\n  author    = \"Sarma Borah, Proyash Paban and Kashyap, Devraj and Laskar,\n               Ruhini Aktar and Sarmah, Ankur Jyoti\",\n  journal   = \"J. Phys. Conf. Ser.\",\n  publisher = \"IOP Publishing\",\n  volume    =  2919,\n  number    =  1,\n  pages     = \"012045\",\n  month     =  dec,\n  year      =  2024,\n  copyright = \"https://creativecommons.org/licenses/by/4.0/\"\n}\n</code></pre>"},{"location":"functions/","title":"Functions","text":""},{"location":"functions/#main-functions","title":"Main Functions","text":"<p>The core functions that can be used to visualise the different Class Activated Mapping(CAM) are given below.  </p>"},{"location":"functions/#YOLOv8_Explainer.core.yolov8_heatmap","title":"<code>yolov8_heatmap</code>","text":"<p>This class is used to implement the YOLOv8 target layer.</p> <p>Args:         weight (str): The path to the checkpoint file.         device (str): The device to use for inference. Defaults to \"cuda:0\" if a GPU is available, otherwise \"cpu\".         method (str): The method to use for computing the CAM. Defaults to \"EigenGradCAM\".         layer (list): The indices of the layers to use for computing the CAM. Defaults to [10, 12, 14, 16, 18, -3].         conf_threshold (float): The confidence threshold for detections. Defaults to 0.2.         ratio (float): The ratio of maximum scores to return. Defaults to 0.02.         show_box (bool): Whether to show bounding boxes with the CAM. Defaults to True.         renormalize (bool): Whether to renormalize the CAM to be in the range [0, 1] across the entire image. Defaults to False.</p> <p>Returns:</p> Type Description <p>A tensor containing the output.</p> Source code in <code>YOLOv8_Explainer/core.py</code> <pre><code>class yolov8_heatmap:\n    \"\"\"\n    This class is used to implement the YOLOv8 target layer.\n\n     Args:\n            weight (str): The path to the checkpoint file.\n            device (str): The device to use for inference. Defaults to \"cuda:0\" if a GPU is available, otherwise \"cpu\".\n            method (str): The method to use for computing the CAM. Defaults to \"EigenGradCAM\".\n            layer (list): The indices of the layers to use for computing the CAM. Defaults to [10, 12, 14, 16, 18, -3].\n            conf_threshold (float): The confidence threshold for detections. Defaults to 0.2.\n            ratio (float): The ratio of maximum scores to return. Defaults to 0.02.\n            show_box (bool): Whether to show bounding boxes with the CAM. Defaults to True.\n            renormalize (bool): Whether to renormalize the CAM to be in the range [0, 1] across the entire image. Defaults to False.\n\n    Returns:\n        A tensor containing the output.\n\n    \"\"\"\n\n    def __init__(\n            self,\n            weight: str,\n            device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n            method=\"EigenGradCAM\",\n            layer=[12, 17, 21],\n            conf_threshold=0.2,\n            ratio=0.02,\n            show_box=True,\n            renormalize=False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the YOLOv8 heatmap layer.\n        \"\"\"\n        device = device\n        backward_type = \"all\"\n        ckpt = torch.load(weight)\n        model_names = ckpt['model'].names\n        model = attempt_load_weights(weight, device)\n        model.info()\n        for p in model.parameters():\n            p.requires_grad_(True)\n        model.eval()\n\n        target = yolov8_target(backward_type, conf_threshold, ratio)\n        target_layers = [model.model[l] for l in layer]\n\n        method = eval(method)(model, target_layers,\n                              use_cuda=device.type == 'cuda')\n        method.activations_and_grads = ActivationsAndGradients(\n            model, target_layers, None)\n\n        colors = np.random.uniform(\n            0, 255, size=(len(model_names), 3)).astype(int)\n        self.__dict__.update(locals())\n\n    def post_process(self, result):\n        \"\"\"\n        Perform non-maximum suppression on the detections and process results.\n\n        Args:\n            result (torch.Tensor): The raw detections from the model.\n\n        Returns:\n            torch.Tensor: Filtered and processed detections.\n        \"\"\"\n        # Perform non-maximum suppression\n        processed_result = non_max_suppression(\n            result,\n            conf_thres=self.conf_threshold,  # Use the class's confidence threshold\n            iou_thres=0.45  # Intersection over Union threshold\n        )\n\n        # If no detections, return an empty tensor\n        if len(processed_result) == 0 or processed_result[0].numel() == 0:\n            return torch.empty(0, 6)  # Return an empty tensor with 6 columns\n\n        # Take the first batch of detections (assuming single image)\n        detections = processed_result[0]\n\n        # Filter detections based on confidence\n        mask = detections[:, 4] &gt;= self.conf_threshold\n        filtered_detections = detections[mask]\n\n        return filtered_detections\n\n    def draw_detections(self, box, color, name, img):\n        \"\"\"\n        Draw bounding boxes and labels on an image for multiple detections.\n\n        Args:\n            box (torch.Tensor or np.ndarray): The bounding box coordinates in the format [x1, y1, x2, y2]\n            color (list): The color of the bounding box in the format [B, G, R]\n            name (str): The label for the bounding box.\n            img (np.ndarray): The image on which to draw the bounding box\n\n        Returns:\n            np.ndarray: The image with the bounding box drawn.\n        \"\"\"\n        # Ensure box coordinates are integers\n        xmin, ymin, xmax, ymax = map(int, box[:4])\n\n        # Draw rectangle\n        cv2.rectangle(img, (xmin, ymin), (xmax, ymax),\n                      tuple(int(x) for x in color), 2)\n\n        # Draw label\n        cv2.putText(img, name, (xmin, ymin - 5),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.8, tuple(int(x) for x in color), 2,\n                    lineType=cv2.LINE_AA)\n\n        return img\n\n    def renormalize_cam_in_bounding_boxes(\n            self,\n            boxes: np.ndarray,  # type: ignore\n            image_float_np: np.ndarray,  # type: ignore\n            grayscale_cam: np.ndarray,  # type: ignore\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Normalize the CAM to be in the range [0, 1]\n        inside every bounding boxes, and zero outside of the bounding boxes.\n\n        Args:\n            boxes (np.ndarray): The bounding boxes.\n            image_float_np (np.ndarray): The image as a numpy array of floats in the range [0, 1].\n            grayscale_cam (np.ndarray): The CAM as a numpy array of floats in the range [0, 1].\n\n        Returns:\n            np.ndarray: The renormalized CAM.\n        \"\"\"\n        renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n        for x1, y1, x2, y2 in boxes:\n            x1, y1 = max(x1, 0), max(y1, 0)\n            x2, y2 = min(grayscale_cam.shape[1] - 1,\n                         x2), min(grayscale_cam.shape[0] - 1, y2)\n            renormalized_cam[y1:y2, x1:x2] = scale_cam_image(\n                grayscale_cam[y1:y2, x1:x2].copy())\n        renormalized_cam = scale_cam_image(renormalized_cam)\n        eigencam_image_renormalized = show_cam_on_image(\n            image_float_np, renormalized_cam, use_rgb=True)\n        return eigencam_image_renormalized\n\n    def renormalize_cam(self, boxes, image_float_np, grayscale_cam):\n        \"\"\"Normalize the CAM to be in the range [0, 1]\n        across the entire image.\"\"\"\n        renormalized_cam = scale_cam_image(grayscale_cam)\n        eigencam_image_renormalized = show_cam_on_image(\n            image_float_np, renormalized_cam, use_rgb=True)\n        return eigencam_image_renormalized\n\n    def process(self, img_path):\n        \"\"\"Process the input image and generate CAM visualization.\"\"\"\n        img = cv2.imread(img_path)\n        img = letterbox(img)[0]\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = np.float32(img) / 255.0\n\n        tensor = (\n            torch.from_numpy(np.transpose(img, axes=[2, 0, 1]))\n            .unsqueeze(0)\n            .to(self.device)\n        )\n\n        try:\n            grayscale_cam = self.method(tensor, [self.target])\n        except AttributeError as e:\n            print(e)\n            return\n\n        grayscale_cam = grayscale_cam[0, :]\n\n        pred1 = self.model(tensor)[0]\n        pred = non_max_suppression(\n            pred1,\n            conf_thres=self.conf_threshold,\n            iou_thres=0.45\n        )[0]\n\n        # Debugging print\n\n        if self.renormalize:\n            cam_image = self.renormalize_cam(\n                pred[:, :4].cpu().detach().numpy().astype(np.int32),\n                img,\n                grayscale_cam\n            )\n        else:\n            cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n\n        if self.show_box and len(pred) &gt; 0:\n            for detection in pred:\n                detection = detection.cpu().detach().numpy()\n\n                # Get class index and confidence\n                class_index = int(detection[5])\n                conf = detection[4]\n\n                # Draw detection\n                cam_image = self.draw_detections(\n                    detection[:4],  # Box coordinates\n                    self.colors[class_index],  # Color for this class\n                    f\"{self.model_names[class_index]}\",  # Label with confidence\n                    cam_image,\n                )\n\n        cam_image = Image.fromarray(cam_image)\n        return cam_image\n\n    def __call__(self, img_path):\n        \"\"\"Generate CAM visualizations for one or more images.\n\n        Args:\n            img_path (str): Path to the input image or directory containing images.\n\n        Returns:\n            None\n        \"\"\"\n        if os.path.isdir(img_path):\n            image_list = []\n            for img_path_ in os.listdir(img_path):\n                img_pil = self.process(f\"{img_path}/{img_path_}\")\n                image_list.append(img_pil)\n            return image_list\n        else:\n            return [self.process(img_path)]\n</code></pre>"},{"location":"functions/#YOLOv8_Explainer.core.yolov8_heatmap.__call__","title":"<code>__call__(img_path)</code>","text":"<p>Generate CAM visualizations for one or more images.</p> <p>Parameters:</p> Name Type Description Default <code>img_path</code> <code>str</code> <p>Path to the input image or directory containing images.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>YOLOv8_Explainer/core.py</code> <pre><code>def __call__(self, img_path):\n    \"\"\"Generate CAM visualizations for one or more images.\n\n    Args:\n        img_path (str): Path to the input image or directory containing images.\n\n    Returns:\n        None\n    \"\"\"\n    if os.path.isdir(img_path):\n        image_list = []\n        for img_path_ in os.listdir(img_path):\n            img_pil = self.process(f\"{img_path}/{img_path_}\")\n            image_list.append(img_pil)\n        return image_list\n    else:\n        return [self.process(img_path)]\n</code></pre>"},{"location":"functions/#YOLOv8_Explainer.core.yolov8_heatmap.__init__","title":"<code>__init__(weight, device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'), method='EigenGradCAM', layer=[12, 17, 21], conf_threshold=0.2, ratio=0.02, show_box=True, renormalize=False)</code>","text":"<p>Initialize the YOLOv8 heatmap layer.</p> Source code in <code>YOLOv8_Explainer/core.py</code> <pre><code>def __init__(\n        self,\n        weight: str,\n        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n        method=\"EigenGradCAM\",\n        layer=[12, 17, 21],\n        conf_threshold=0.2,\n        ratio=0.02,\n        show_box=True,\n        renormalize=False,\n) -&gt; None:\n    \"\"\"\n    Initialize the YOLOv8 heatmap layer.\n    \"\"\"\n    device = device\n    backward_type = \"all\"\n    ckpt = torch.load(weight)\n    model_names = ckpt['model'].names\n    model = attempt_load_weights(weight, device)\n    model.info()\n    for p in model.parameters():\n        p.requires_grad_(True)\n    model.eval()\n\n    target = yolov8_target(backward_type, conf_threshold, ratio)\n    target_layers = [model.model[l] for l in layer]\n\n    method = eval(method)(model, target_layers,\n                          use_cuda=device.type == 'cuda')\n    method.activations_and_grads = ActivationsAndGradients(\n        model, target_layers, None)\n\n    colors = np.random.uniform(\n        0, 255, size=(len(model_names), 3)).astype(int)\n    self.__dict__.update(locals())\n</code></pre>"},{"location":"functions/#YOLOv8_Explainer.core.yolov8_heatmap.draw_detections","title":"<code>draw_detections(box, color, name, img)</code>","text":"<p>Draw bounding boxes and labels on an image for multiple detections.</p> <p>Parameters:</p> Name Type Description Default <code>box</code> <code>Tensor or ndarray</code> <p>The bounding box coordinates in the format [x1, y1, x2, y2]</p> required <code>color</code> <code>list</code> <p>The color of the bounding box in the format [B, G, R]</p> required <code>name</code> <code>str</code> <p>The label for the bounding box.</p> required <code>img</code> <code>ndarray</code> <p>The image on which to draw the bounding box</p> required <p>Returns:</p> Type Description <p>np.ndarray: The image with the bounding box drawn.</p> Source code in <code>YOLOv8_Explainer/core.py</code> <pre><code>def draw_detections(self, box, color, name, img):\n    \"\"\"\n    Draw bounding boxes and labels on an image for multiple detections.\n\n    Args:\n        box (torch.Tensor or np.ndarray): The bounding box coordinates in the format [x1, y1, x2, y2]\n        color (list): The color of the bounding box in the format [B, G, R]\n        name (str): The label for the bounding box.\n        img (np.ndarray): The image on which to draw the bounding box\n\n    Returns:\n        np.ndarray: The image with the bounding box drawn.\n    \"\"\"\n    # Ensure box coordinates are integers\n    xmin, ymin, xmax, ymax = map(int, box[:4])\n\n    # Draw rectangle\n    cv2.rectangle(img, (xmin, ymin), (xmax, ymax),\n                  tuple(int(x) for x in color), 2)\n\n    # Draw label\n    cv2.putText(img, name, (xmin, ymin - 5),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.8, tuple(int(x) for x in color), 2,\n                lineType=cv2.LINE_AA)\n\n    return img\n</code></pre>"},{"location":"functions/#YOLOv8_Explainer.core.yolov8_heatmap.post_process","title":"<code>post_process(result)</code>","text":"<p>Perform non-maximum suppression on the detections and process results.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Tensor</code> <p>The raw detections from the model.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Filtered and processed detections.</p> Source code in <code>YOLOv8_Explainer/core.py</code> <pre><code>def post_process(self, result):\n    \"\"\"\n    Perform non-maximum suppression on the detections and process results.\n\n    Args:\n        result (torch.Tensor): The raw detections from the model.\n\n    Returns:\n        torch.Tensor: Filtered and processed detections.\n    \"\"\"\n    # Perform non-maximum suppression\n    processed_result = non_max_suppression(\n        result,\n        conf_thres=self.conf_threshold,  # Use the class's confidence threshold\n        iou_thres=0.45  # Intersection over Union threshold\n    )\n\n    # If no detections, return an empty tensor\n    if len(processed_result) == 0 or processed_result[0].numel() == 0:\n        return torch.empty(0, 6)  # Return an empty tensor with 6 columns\n\n    # Take the first batch of detections (assuming single image)\n    detections = processed_result[0]\n\n    # Filter detections based on confidence\n    mask = detections[:, 4] &gt;= self.conf_threshold\n    filtered_detections = detections[mask]\n\n    return filtered_detections\n</code></pre>"},{"location":"functions/#YOLOv8_Explainer.core.yolov8_heatmap.process","title":"<code>process(img_path)</code>","text":"<p>Process the input image and generate CAM visualization.</p> Source code in <code>YOLOv8_Explainer/core.py</code> <pre><code>def process(self, img_path):\n    \"\"\"Process the input image and generate CAM visualization.\"\"\"\n    img = cv2.imread(img_path)\n    img = letterbox(img)[0]\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = np.float32(img) / 255.0\n\n    tensor = (\n        torch.from_numpy(np.transpose(img, axes=[2, 0, 1]))\n        .unsqueeze(0)\n        .to(self.device)\n    )\n\n    try:\n        grayscale_cam = self.method(tensor, [self.target])\n    except AttributeError as e:\n        print(e)\n        return\n\n    grayscale_cam = grayscale_cam[0, :]\n\n    pred1 = self.model(tensor)[0]\n    pred = non_max_suppression(\n        pred1,\n        conf_thres=self.conf_threshold,\n        iou_thres=0.45\n    )[0]\n\n    # Debugging print\n\n    if self.renormalize:\n        cam_image = self.renormalize_cam(\n            pred[:, :4].cpu().detach().numpy().astype(np.int32),\n            img,\n            grayscale_cam\n        )\n    else:\n        cam_image = show_cam_on_image(img, grayscale_cam, use_rgb=True)\n\n    if self.show_box and len(pred) &gt; 0:\n        for detection in pred:\n            detection = detection.cpu().detach().numpy()\n\n            # Get class index and confidence\n            class_index = int(detection[5])\n            conf = detection[4]\n\n            # Draw detection\n            cam_image = self.draw_detections(\n                detection[:4],  # Box coordinates\n                self.colors[class_index],  # Color for this class\n                f\"{self.model_names[class_index]}\",  # Label with confidence\n                cam_image,\n            )\n\n    cam_image = Image.fromarray(cam_image)\n    return cam_image\n</code></pre>"},{"location":"functions/#YOLOv8_Explainer.core.yolov8_heatmap.renormalize_cam","title":"<code>renormalize_cam(boxes, image_float_np, grayscale_cam)</code>","text":"<p>Normalize the CAM to be in the range [0, 1] across the entire image.</p> Source code in <code>YOLOv8_Explainer/core.py</code> <pre><code>def renormalize_cam(self, boxes, image_float_np, grayscale_cam):\n    \"\"\"Normalize the CAM to be in the range [0, 1]\n    across the entire image.\"\"\"\n    renormalized_cam = scale_cam_image(grayscale_cam)\n    eigencam_image_renormalized = show_cam_on_image(\n        image_float_np, renormalized_cam, use_rgb=True)\n    return eigencam_image_renormalized\n</code></pre>"},{"location":"functions/#YOLOv8_Explainer.core.yolov8_heatmap.renormalize_cam_in_bounding_boxes","title":"<code>renormalize_cam_in_bounding_boxes(boxes, image_float_np, grayscale_cam)</code>","text":"<p>Normalize the CAM to be in the range [0, 1] inside every bounding boxes, and zero outside of the bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>boxes</code> <code>ndarray</code> <p>The bounding boxes.</p> required <code>image_float_np</code> <code>ndarray</code> <p>The image as a numpy array of floats in the range [0, 1].</p> required <code>grayscale_cam</code> <code>ndarray</code> <p>The CAM as a numpy array of floats in the range [0, 1].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The renormalized CAM.</p> Source code in <code>YOLOv8_Explainer/core.py</code> <pre><code>def renormalize_cam_in_bounding_boxes(\n        self,\n        boxes: np.ndarray,  # type: ignore\n        image_float_np: np.ndarray,  # type: ignore\n        grayscale_cam: np.ndarray,  # type: ignore\n) -&gt; np.ndarray:\n    \"\"\"\n    Normalize the CAM to be in the range [0, 1]\n    inside every bounding boxes, and zero outside of the bounding boxes.\n\n    Args:\n        boxes (np.ndarray): The bounding boxes.\n        image_float_np (np.ndarray): The image as a numpy array of floats in the range [0, 1].\n        grayscale_cam (np.ndarray): The CAM as a numpy array of floats in the range [0, 1].\n\n    Returns:\n        np.ndarray: The renormalized CAM.\n    \"\"\"\n    renormalized_cam = np.zeros(grayscale_cam.shape, dtype=np.float32)\n    for x1, y1, x2, y2 in boxes:\n        x1, y1 = max(x1, 0), max(y1, 0)\n        x2, y2 = min(grayscale_cam.shape[1] - 1,\n                     x2), min(grayscale_cam.shape[0] - 1, y2)\n        renormalized_cam[y1:y2, x1:x2] = scale_cam_image(\n            grayscale_cam[y1:y2, x1:x2].copy())\n    renormalized_cam = scale_cam_image(renormalized_cam)\n    eigencam_image_renormalized = show_cam_on_image(\n        image_float_np, renormalized_cam, use_rgb=True)\n    return eigencam_image_renormalized\n</code></pre>"},{"location":"functions/#helper-functions","title":"Helper Functions","text":"<p>The functions that can be used to display images and provide various other functionalities can be found here. </p>"},{"location":"functions/#YOLOv8_Explainer.utils.display_images","title":"<code>display_images(images)</code>","text":"<p>Display a list of PIL images in a grid.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[Image]</code> <p>A list of PIL images to display.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>YOLOv8_Explainer/utils.py</code> <pre><code>def display_images(images):\n    \"\"\"\n    Display a list of PIL images in a grid.\n\n    Args:\n        images (list[PIL.Image]): A list of PIL images to display.\n\n    Returns:\n        None\n    \"\"\"\n    fig, axes = plt.subplots(1, len(images), figsize=(15, 7))\n    if len(images) == 1:\n        axes = [axes]\n    for ax, img in zip(axes, images):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.show()\n</code></pre>"},{"location":"functions/#YOLOv8_Explainer.utils.letterbox","title":"<code>letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32)</code>","text":"<p>Resize and pad image while meeting stride-multiple constraints.</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>ndarray</code> <p>Input image.</p> required <code>new_shape</code> <code>tuple</code> <p>Desired output shape. Defaults to (640, 640).</p> <code>(640, 640)</code> <code>color</code> <code>tuple</code> <p>Color of the border. Defaults to (114, 114, 114).</p> <code>(114, 114, 114)</code> <code>auto</code> <code>bool</code> <p>Whether to automatically determine padding. Defaults to True.</p> <code>True</code> <code>scaleFill</code> <code>bool</code> <p>Whether to stretch the image to fill the new shape. Defaults to False.</p> <code>False</code> <code>scaleup</code> <code>bool</code> <p>Whether to scale the image up if necessary. Defaults to True.</p> <code>True</code> <code>stride</code> <code>int</code> <p>Stride of the sliding window. Defaults to 32.</p> <code>32</code> <p>Returns:</p> Name Type Description <p>numpy.ndarray: Letterboxed image.</p> <code>tuple</code> <p>Ratio of the resized image.</p> <code>tuple</code> <p>Padding sizes.</p> Source code in <code>YOLOv8_Explainer/utils.py</code> <pre><code>def letterbox(\n    im: np.ndarray,\n    new_shape=(640, 640),\n    color=(114, 114, 114),\n    auto=True,\n    scaleFill=False,\n    scaleup=True,\n    stride=32,\n):\n    \"\"\"\n    Resize and pad image while meeting stride-multiple constraints.\n\n    Args:\n        im (numpy.ndarray): Input image.\n        new_shape (tuple, optional): Desired output shape. Defaults to (640, 640).\n        color (tuple, optional): Color of the border. Defaults to (114, 114, 114).\n        auto (bool, optional): Whether to automatically determine padding. Defaults to True.\n        scaleFill (bool, optional): Whether to stretch the image to fill the new shape. Defaults to False.\n        scaleup (bool, optional): Whether to scale the image up if necessary. Defaults to True.\n        stride (int, optional): Stride of the sliding window. Defaults to 32.\n\n    Returns:\n        numpy.ndarray: Letterboxed image.\n        tuple: Ratio of the resized image.\n        tuple: Padding sizes.\n\n    \"\"\"\n    shape = im.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        new_shape = (new_shape, new_shape)\n\n    # Scale ratio (new / old)\n    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n        r = min(r, 1.0)\n\n    # Compute padding\n    ratio = r, r  # width, height ratios\n    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n    if auto:  # minimum rectangle\n        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n    elif scaleFill:  # stretch\n        dw, dh = 0.0, 0.0\n        new_unpad = (new_shape[1], new_shape[0])\n        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n\n    dw /= 2  # divide padding into 2 sides\n    dh /= 2\n\n    if shape[::-1] != new_unpad:  # resize\n        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n\n    return im, ratio, (dw, dh)\n</code></pre>"},{"location":"references/","title":"References","text":"<p>https://github.com/jacobgil/pytorch-grad-cam  <code>PyTorch library for CAM methods Jacob Gildenblat and contributors</code></p> <p>https://github.com/z1069614715/objectdetection_script  <code>Object Detection Script Devil's Mask</code></p> <p>https://arxiv.org/abs/1610.02391  <code>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra</code></p> <p>https://arxiv.org/abs/2011.08891  <code>Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks Rachel L. Draelos, Lawrence Carin</code></p> <p>https://arxiv.org/abs/1710.11063  <code>Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader, Vineeth N Balasubramanian</code></p> <p>https://arxiv.org/abs/1910.01279  <code>Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks Haofan Wang, Zifan Wang, Mengnan Du, Fan Yang, Zijian Zhang, Sirui Ding, Piotr Mardziel, Xia Hu</code></p> <p>https://ieeexplore.ieee.org/abstract/document/9093360/  <code>Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization. Saurabh Desai and Harish G Ramaswamy. In WACV, pages 972\u2013980, 2020</code></p> <p>https://arxiv.org/abs/2008.02312  <code>Axiom-based Grad-CAM: Towards Accurate Visualization and Explanation of CNNs Ruigang Fu, Qingyong Hu, Xiaohu Dong, Yulan Guo, Yinghui Gao, Biao Li</code></p> <p>https://arxiv.org/abs/2008.00299  <code>Eigen-CAM: Class Activation Map using Principal Components Mohammed Bany Muhammad, Mohammed Yeasin</code></p> <p>http://mftp.mmcheng.net/Papers/21TIP_LayerCAM.pdf  <code>LayerCAM: Exploring Hierarchical Class Activation Maps for Localization Peng-Tao Jiang; Chang-Bin Zhang; Qibin Hou; Ming-Ming Cheng; Yunchao Wei</code></p> <p>https://arxiv.org/abs/1905.00780  <code>Full-Gradient Representation for Neural Network Visualization Suraj Srinivas, Francois Fleuret</code></p> <p>https://arxiv.org/abs/1806.10206  <code>Deep Feature Factorization For Concept Discovery Edo Collins, Radhakrishna Achanta, Sabine S\u00fcsstrunk</code></p>"}]}